{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 - Probability threshold otimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import defaultdict \n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import SVC\n",
    "from utils import *\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from collections import Counter\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age at Injury', 'Alternative Dispute Resolution',\n",
       "       'Attorney/Representative', 'Carrier Name', 'Carrier Type',\n",
       "       'Claim Injury Type', 'County of Injury', 'COVID-19 Indicator',\n",
       "       'District Name', 'Gender', 'IME-4 Count', 'Industry Code Description',\n",
       "       'Medical Fee Region', 'WCIO Cause of Injury Description',\n",
       "       'WCIO Nature of Injury Description', 'WCIO Part Of Body Description',\n",
       "       'Zip Code', 'Agreement Reached', 'WCB Decision', 'Number of Dependents',\n",
       "       'Accident Date_year', 'Accident Date_missing', 'Accident_weekend',\n",
       "       'Accident Date_month_cos', 'Accident Date_month_sin',\n",
       "       'Accident Date_quarter_cos', 'Accident Date_quarter_sin',\n",
       "       'Accident Date_assembly_gap_days', 'C3-C2_gap_days', 'C2_missing',\n",
       "       'C3_missing', 'C2_Accident_gap_weeks', 'C3_Accident_gap_weeks',\n",
       "       'Hearing Date_missing', 'Hearing_C3 gap_months',\n",
       "       'Hearing_C2 gap_months', 'Hearing_assembly_gap_months',\n",
       "       'Days to Assembly', 'Days to First Hearing', 'Days from COVID',\n",
       "       'Age_not_correct', 'Average Weekly Wage_log', 'Work_on_distance',\n",
       "       'target_1', 'target_2', 'target_3', 'target_4', 'target_5', 'target_6',\n",
       "       'target_7', 'target_8', 'Claim Injury Type_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"train_new_feats.csv\")\n",
    "\n",
    "\n",
    "target = [[f\"target_{i}\" for i in range(1, 9)] + [\"Claim Injury Type\"] + [\"WCB Decision\"] + [\"Agreement Reached\"] + [\"Claim Injury Type_encoded\"]]\n",
    "target = [item for sublist in target for item in sublist]\n",
    "target\n",
    "\n",
    "binary_target = [f\"target_{i}\" for i in range(1, 9)]\n",
    "\n",
    "original_target  = [col for col in target if col not in binary_target]\n",
    "\n",
    "ordinal_target = [\"Claim Injury Type_encoded\"]\n",
    "\n",
    "features = [feat for feat in df.columns if feat not in target]\n",
    "\n",
    "features = [feat for feat in features if df[feat].dtype != \"datetime64[ns]\"]\n",
    "\n",
    "num_feats = [feat for feat in features if df[feat].dtype != \"object\"]\n",
    "\n",
    "cat_feats = [feat for feat in features if df[feat].dtype == \"object\"]\n",
    "cat_feats_index = [features.index(feat) for feat in cat_feats]\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_imputing(X_train, X_val):\n",
    "    feats_imput_max = [\"C2_Accident_gap_weeks\", \"C3_Accident_gap_weeks\", \"Accident Date_assembly_gap_days\", \"Hearing_C3 gap_months\", \"Hearing_C2 gap_months\", \"Hearing_assembly_gap_months\", \"Days to First Hearing\"]\n",
    "\n",
    "    feat_imput_min = [\"C3-C2_gap_days\"]\n",
    "    \n",
    "    for feat in X_train.columns:\n",
    "        if X_train[feat].isna().sum() > 0 or X_val[feat].isna().sum() > 0:\n",
    "            if feat in feats_imput_max:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].max())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].max())\n",
    "            elif feat in feat_imput_min:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].min())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].min())\n",
    "            else:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].mean())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].mean())\n",
    "    return X_train, X_val\n",
    "\n",
    "def frequency_encoding(train_df, val_df, column):\n",
    "    \"\"\"\n",
    "    Apply frequency encoding on the training set and use the same encoding to impute the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training dataset.\n",
    "    val_df (pd.DataFrame): Validation dataset.\n",
    "    column (str): Column to encode.\n",
    "    \n",
    "    Returns:\n",
    "    train_encoded (pd.DataFrame): Encoded training set.\n",
    "    val_encoded (pd.DataFrame): Encoded validation set.\n",
    "    freq_map (dict): Mapping of frequency counts for the column.\n",
    "    \"\"\"\n",
    "    # Compute frequency encoding for the training set\n",
    "    freq_map = train_df[column].value_counts(normalize=True)  # Relative frequency\n",
    "    train_df[f\"{column}_freq\"] = train_df[column].map(freq_map)\n",
    "\n",
    "    # Impute frequency encoding on the validation set using the same mapping\n",
    "    val_df[f\"{column}_freq\"] = val_df[column].map(freq_map)\n",
    "\n",
    "    # Handle unseen categories in validation by imputing 0 frequency\n",
    "    val_df[f\"{column}_freq\"] = val_df[f\"{column}_freq\"].fillna(0)\n",
    "    \n",
    "    train_df = train_df.drop(columns=[column])\n",
    "    val_df = val_df.drop(columns=[column])\n",
    "\n",
    "    # Return encoded datasets and frequency map\n",
    "    return train_df, val_df, freq_map\n",
    "\n",
    "def target_guided_ordinal_encoding(X_train, X_val, categorical_column, target_column, y_train, i):\n",
    "    # Combine X_train with y_train temporarily to calculate means\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    X_train_encoded[target_column] = y_train\n",
    "\n",
    "    means = X_train_encoded.groupby(categorical_column)[target_column].mean()\n",
    "    #print(means)\n",
    "\n",
    "    sorted_means = means.sort_values(by=target_column)\n",
    "    #print(sorted_means)\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing sorted means for {categorical_column}\")\n",
    "    #     lst_names = sorted_means.index.tolist()\n",
    "    #     lst_values = sorted_means.values.tolist()\n",
    "    #     dict_final = dict(zip(lst_names, lst_values))\n",
    "    #     print(dict_final)\n",
    "    \n",
    "    ordinal_mapping = {category: rank for rank, category in enumerate(sorted_means.index, start=1)}\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing ordinal mapping for {categorical_column}\")\n",
    "    #     print(ordinal_mapping)\n",
    "    #     print(\"--------------------------------\")\n",
    "        \n",
    "    X_train_encoded[f\"{categorical_column}_encoded\"] = X_train_encoded[categorical_column].map(ordinal_mapping)\n",
    "    X_val_encoded[f\"{categorical_column}_encoded\"] = X_val_encoded[categorical_column].map(ordinal_mapping)\n",
    "\n",
    "    #X_train_encoded = X_train_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.drop(columns=[target_column[0]])\n",
    "    #X_val_encoded = X_val_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.fillna(1)\n",
    "    X_val_encoded = X_val_encoded.fillna(1)\n",
    "\n",
    "    return X_train_encoded, X_val_encoded, ordinal_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features=['Attorney/Representative',\n",
    " 'IME-4 Count',\n",
    " 'Accident Date_year',\n",
    " 'Accident Date_assembly_gap_days',\n",
    " 'C3-C2_gap_days',\n",
    " 'C2_missing',\n",
    " 'C3_missing',\n",
    " 'C3_Accident_gap_weeks',\n",
    " 'Hearing_C3 gap_months',\n",
    " 'Hearing_C2 gap_months',\n",
    " 'Days to Assembly',\n",
    " 'Days to First Hearing',\n",
    " 'Average Weekly Wage_log',\n",
    " 'Carrier Name_encoded',\n",
    " 'Carrier Type_encoded',\n",
    " 'Industry Code Description_encoded',\n",
    " 'WCIO Cause of Injury Description_encoded',\n",
    " 'WCIO Nature of Injury Description_encoded',\n",
    " 'WCIO Part Of Body Description_encoded',\n",
    " 'Carrier Name_freq',\n",
    " 'Carrier Type_freq',\n",
    " 'Industry Code Description_freq',\n",
    " 'WCIO Nature of Injury Description_freq',\n",
    " 'WCIO Part Of Body Description_freq']\n",
    "\n",
    "naive_features = [feat.replace(\"_encoded\", \"\") for feat in selected_features]\n",
    "naive_features = [feat.replace(f\"_freq\", \"\") for feat in naive_features]\n",
    "naive_features = set(naive_features)\n",
    "naive_features = list(naive_features)\n",
    "\n",
    "cat_feats = [feat for feat in naive_features if feat in cat_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[naive_features]\n",
    "y = df[ordinal_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding...\n",
      "Frequency encoding...\n",
      "0:\tlearn: 0.6714469\ttotal: 2.18s\tremaining: 36m 15s\n",
      "10:\tlearn: 0.5094364\ttotal: 17.6s\tremaining: 26m 24s\n",
      "20:\tlearn: 0.4100677\ttotal: 34s\tremaining: 26m 23s\n",
      "30:\tlearn: 0.3451700\ttotal: 49.9s\tremaining: 25m 59s\n",
      "40:\tlearn: 0.3014844\ttotal: 1m 2s\tremaining: 24m 33s\n",
      "50:\tlearn: 0.2707459\ttotal: 1m 18s\tremaining: 24m 22s\n",
      "60:\tlearn: 0.2491847\ttotal: 1m 37s\tremaining: 24m 58s\n",
      "70:\tlearn: 0.2334113\ttotal: 1m 54s\tremaining: 24m 52s\n",
      "80:\tlearn: 0.2220211\ttotal: 2m 9s\tremaining: 24m 31s\n",
      "90:\tlearn: 0.2135865\ttotal: 2m 25s\tremaining: 24m 16s\n",
      "100:\tlearn: 0.2071378\ttotal: 2m 39s\tremaining: 23m 39s\n",
      "110:\tlearn: 0.2022865\ttotal: 2m 49s\tremaining: 22m 34s\n",
      "120:\tlearn: 0.1985600\ttotal: 2m 59s\tremaining: 21m 44s\n",
      "130:\tlearn: 0.1954808\ttotal: 3m 13s\tremaining: 21m 22s\n",
      "140:\tlearn: 0.1929047\ttotal: 3m 25s\tremaining: 20m 54s\n",
      "150:\tlearn: 0.1906462\ttotal: 3m 39s\tremaining: 20m 31s\n",
      "160:\tlearn: 0.1888497\ttotal: 3m 51s\tremaining: 20m 3s\n",
      "170:\tlearn: 0.1873172\ttotal: 4m 3s\tremaining: 19m 38s\n",
      "180:\tlearn: 0.1859830\ttotal: 4m 17s\tremaining: 19m 22s\n",
      "190:\tlearn: 0.1849663\ttotal: 4m 28s\tremaining: 18m 58s\n",
      "200:\tlearn: 0.1838739\ttotal: 4m 39s\tremaining: 18m 31s\n",
      "210:\tlearn: 0.1829085\ttotal: 4m 49s\tremaining: 18m\n",
      "220:\tlearn: 0.1820890\ttotal: 5m 3s\tremaining: 17m 51s\n",
      "230:\tlearn: 0.1812424\ttotal: 5m 15s\tremaining: 17m 29s\n",
      "240:\tlearn: 0.1804713\ttotal: 5m 26s\tremaining: 17m 7s\n",
      "250:\tlearn: 0.1798351\ttotal: 5m 39s\tremaining: 16m 53s\n",
      "260:\tlearn: 0.1792324\ttotal: 5m 54s\tremaining: 16m 44s\n",
      "270:\tlearn: 0.1787071\ttotal: 6m 7s\tremaining: 16m 29s\n",
      "280:\tlearn: 0.1782201\ttotal: 6m 19s\tremaining: 16m 12s\n",
      "290:\tlearn: 0.1777783\ttotal: 6m 30s\tremaining: 15m 52s\n",
      "300:\tlearn: 0.1772980\ttotal: 6m 40s\tremaining: 15m 30s\n",
      "310:\tlearn: 0.1768721\ttotal: 6m 52s\tremaining: 15m 14s\n",
      "320:\tlearn: 0.1764772\ttotal: 7m 2s\tremaining: 14m 54s\n",
      "330:\tlearn: 0.1761161\ttotal: 7m 15s\tremaining: 14m 39s\n",
      "340:\tlearn: 0.1757335\ttotal: 7m 27s\tremaining: 14m 25s\n",
      "350:\tlearn: 0.1754353\ttotal: 7m 40s\tremaining: 14m 11s\n",
      "360:\tlearn: 0.1750778\ttotal: 7m 51s\tremaining: 13m 53s\n",
      "370:\tlearn: 0.1747734\ttotal: 8m 3s\tremaining: 13m 40s\n",
      "380:\tlearn: 0.1744684\ttotal: 8m 18s\tremaining: 13m 29s\n",
      "390:\tlearn: 0.1741920\ttotal: 8m 30s\tremaining: 13m 15s\n",
      "400:\tlearn: 0.1739043\ttotal: 8m 39s\tremaining: 12m 56s\n",
      "410:\tlearn: 0.1736581\ttotal: 8m 49s\tremaining: 12m 38s\n",
      "420:\tlearn: 0.1734163\ttotal: 9m\tremaining: 12m 23s\n",
      "430:\tlearn: 0.1731487\ttotal: 9m 10s\tremaining: 12m 7s\n",
      "440:\tlearn: 0.1728820\ttotal: 9m 22s\tremaining: 11m 52s\n",
      "450:\tlearn: 0.1726264\ttotal: 9m 33s\tremaining: 11m 37s\n",
      "460:\tlearn: 0.1723861\ttotal: 9m 43s\tremaining: 11m 22s\n",
      "470:\tlearn: 0.1721417\ttotal: 9m 52s\tremaining: 11m 5s\n",
      "480:\tlearn: 0.1718901\ttotal: 10m 2s\tremaining: 10m 49s\n",
      "490:\tlearn: 0.1716729\ttotal: 10m 13s\tremaining: 10m 36s\n",
      "500:\tlearn: 0.1714652\ttotal: 10m 30s\tremaining: 10m 27s\n",
      "510:\tlearn: 0.1712305\ttotal: 10m 44s\tremaining: 10m 17s\n",
      "520:\tlearn: 0.1710135\ttotal: 10m 58s\tremaining: 10m 5s\n",
      "530:\tlearn: 0.1708010\ttotal: 11m 12s\tremaining: 9m 54s\n",
      "540:\tlearn: 0.1705821\ttotal: 11m 26s\tremaining: 9m 42s\n",
      "550:\tlearn: 0.1704266\ttotal: 11m 37s\tremaining: 9m 28s\n",
      "560:\tlearn: 0.1702211\ttotal: 11m 49s\tremaining: 9m 15s\n",
      "570:\tlearn: 0.1700289\ttotal: 12m 4s\tremaining: 9m 4s\n",
      "580:\tlearn: 0.1698490\ttotal: 12m 20s\tremaining: 8m 53s\n",
      "590:\tlearn: 0.1696543\ttotal: 12m 35s\tremaining: 8m 42s\n",
      "600:\tlearn: 0.1694773\ttotal: 12m 49s\tremaining: 8m 30s\n",
      "610:\tlearn: 0.1692932\ttotal: 13m 2s\tremaining: 8m 18s\n",
      "620:\tlearn: 0.1691374\ttotal: 13m 15s\tremaining: 8m 5s\n",
      "630:\tlearn: 0.1689860\ttotal: 13m 32s\tremaining: 7m 55s\n",
      "640:\tlearn: 0.1688636\ttotal: 13m 47s\tremaining: 7m 43s\n",
      "650:\tlearn: 0.1687319\ttotal: 14m 1s\tremaining: 7m 30s\n",
      "660:\tlearn: 0.1685910\ttotal: 14m 14s\tremaining: 7m 18s\n",
      "670:\tlearn: 0.1684255\ttotal: 14m 28s\tremaining: 7m 5s\n",
      "680:\tlearn: 0.1682910\ttotal: 14m 45s\tremaining: 6m 54s\n",
      "690:\tlearn: 0.1681970\ttotal: 15m 1s\tremaining: 6m 43s\n",
      "700:\tlearn: 0.1680601\ttotal: 15m 17s\tremaining: 6m 31s\n",
      "710:\tlearn: 0.1679047\ttotal: 15m 37s\tremaining: 6m 21s\n",
      "720:\tlearn: 0.1677696\ttotal: 15m 55s\tremaining: 6m 9s\n",
      "730:\tlearn: 0.1676727\ttotal: 16m 15s\tremaining: 5m 58s\n",
      "740:\tlearn: 0.1675626\ttotal: 16m 33s\tremaining: 5m 47s\n",
      "750:\tlearn: 0.1674163\ttotal: 16m 49s\tremaining: 5m 34s\n",
      "760:\tlearn: 0.1673050\ttotal: 17m 6s\tremaining: 5m 22s\n",
      "770:\tlearn: 0.1671806\ttotal: 17m 20s\tremaining: 5m 9s\n",
      "780:\tlearn: 0.1670740\ttotal: 17m 37s\tremaining: 4m 56s\n",
      "790:\tlearn: 0.1669301\ttotal: 17m 54s\tremaining: 4m 44s\n",
      "800:\tlearn: 0.1668119\ttotal: 18m 12s\tremaining: 4m 31s\n",
      "810:\tlearn: 0.1667049\ttotal: 18m 29s\tremaining: 4m 18s\n",
      "820:\tlearn: 0.1665924\ttotal: 18m 43s\tremaining: 4m 5s\n",
      "830:\tlearn: 0.1665128\ttotal: 18m 57s\tremaining: 3m 51s\n",
      "840:\tlearn: 0.1664162\ttotal: 19m 12s\tremaining: 3m 37s\n",
      "850:\tlearn: 0.1663234\ttotal: 19m 28s\tremaining: 3m 24s\n",
      "860:\tlearn: 0.1662074\ttotal: 19m 41s\tremaining: 3m 10s\n",
      "870:\tlearn: 0.1660986\ttotal: 19m 54s\tremaining: 2m 56s\n",
      "880:\tlearn: 0.1660177\ttotal: 20m 6s\tremaining: 2m 42s\n",
      "890:\tlearn: 0.1659091\ttotal: 20m 18s\tremaining: 2m 29s\n",
      "900:\tlearn: 0.1658175\ttotal: 20m 29s\tremaining: 2m 15s\n",
      "910:\tlearn: 0.1657270\ttotal: 20m 41s\tremaining: 2m 1s\n",
      "920:\tlearn: 0.1656469\ttotal: 20m 54s\tremaining: 1m 47s\n",
      "930:\tlearn: 0.1655758\ttotal: 21m 6s\tremaining: 1m 33s\n",
      "940:\tlearn: 0.1654700\ttotal: 21m 17s\tremaining: 1m 20s\n",
      "950:\tlearn: 0.1653820\ttotal: 21m 30s\tremaining: 1m 6s\n",
      "960:\tlearn: 0.1652895\ttotal: 21m 41s\tremaining: 52.8s\n",
      "970:\tlearn: 0.1651953\ttotal: 21m 53s\tremaining: 39.2s\n",
      "980:\tlearn: 0.1651272\ttotal: 22m 5s\tremaining: 25.7s\n",
      "990:\tlearn: 0.1650254\ttotal: 22m 17s\tremaining: 12.1s\n",
      "999:\tlearn: 0.1649510\ttotal: 22m 32s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1b4dac19c40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Ordinal encoding...\")\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y_train, 0)\n",
    "\n",
    "print(f\"Frequency encoding...\")\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "X_train_encoded  = X_train_encoded[selected_features]\n",
    "X_val_encoded = X_val_encoded[selected_features]\n",
    "\n",
    "X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = CatBoostClassifier(random_state=42, verbose=10, iterations=1000, depth=6, boosting_type='Ordered', auto_class_weights='SqrtBalanced', loss_function=\"MultiClassOneVsAll\")\n",
    "\n",
    "clf.fit(X_train_imputed, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_initial = clf.predict(X_train_imputed)\n",
    "val_preds_initial = clf.predict(X_val_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model_train_proba = clf.predict_proba(X_train_imputed)\n",
    "first_model_val_proba = clf.predict_proba(X_val_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_thresholds(y_true, probabilities):\n",
    "    best_thresholds = []\n",
    "    for i in range(probabilities.shape[1]):  # Loop over each class\n",
    "        precision, recall, thresholds = precision_recall_curve((y_true == i).astype(int), probabilities[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_thresholds.append(thresholds[np.argmax(f1_scores)])  # Store best threshold\n",
    "    return best_thresholds\n",
    "\n",
    "def predict_with_thresholds(probabilities, thresholds):\n",
    "    weighted_probs = probabilities / np.array(thresholds)  \n",
    "    predictions = np.argmax(weighted_probs, axis=1)  \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5523639280183358,\n",
       " 0.2706995022796855,\n",
       " 0.24118589499033843,\n",
       " 0.2641656406998126,\n",
       " 0.37531474703930623,\n",
       " 0.3281035672210727,\n",
       " 0.5098517158018644,\n",
       " 0.634415672799047]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thresholds = optimize_thresholds(y_val, first_model_val_proba)\n",
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = predict_with_thresholds(first_model_train_proba, best_thresholds)\n",
    "val_predictions = predict_with_thresholds(first_model_val_proba, best_thresholds)\n",
    "\n",
    "f1_score_train = f1_score(y_train, train_predictions, average=\"macro\")\n",
    "f1_score_val = f1_score(y_val, val_predictions, average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 with model.predict: 0.5487136534674067\n",
      "Validation F1 with model.predict: 0.4775971219853013\n",
      "F1 Score Train with adjusted thesholds: 0.5547015091185956\n",
      "F1 Score Validation with adjusted thesholds: 0.48638799912268993\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train F1 with model.predict: {f1_score(y_train, train_preds_initial, average='macro')}\")\n",
    "print(f\"Validation F1 with model.predict: {f1_score(y_val, val_preds_initial, average='macro')}\")\n",
    "print(f\"F1 Score Train with adjusted thesholds: {f1_score_train}\")\n",
    "print(f\"F1 Score Validation with adjusted thesholds: {f1_score_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
