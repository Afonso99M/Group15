{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Model deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.11.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.24->catboost) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import defaultdict \n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import SVC\n",
    "from utils import *\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from collections import Counter\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_new_feats.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [[f\"target_{i}\" for i in range(1, 9)] + [\"Claim Injury Type\"] + [\"WCB Decision\"] + [\"Agreement Reached\"] + [\"Claim Injury Type_encoded\"]]\n",
    "target = [item for sublist in target for item in sublist]\n",
    "target\n",
    "\n",
    "binary_target = [f\"target_{i}\" for i in range(1, 9)]\n",
    "\n",
    "original_target  = [col for col in target if col not in binary_target]\n",
    "\n",
    "ordinal_target = [\"Claim Injury Type_encoded\"]\n",
    "\n",
    "features = [feat for feat in df.columns if feat not in target]\n",
    "\n",
    "features = [feat for feat in features if df[feat].dtype != \"datetime64[ns]\"]\n",
    "\n",
    "num_feats = [feat for feat in features if df[feat].dtype != \"object\"]\n",
    "\n",
    "cat_feats = [feat for feat in features if df[feat].dtype == \"object\"]\n",
    "cat_feats_index = [features.index(feat) for feat in cat_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_imputing(X_train, X_val):\n",
    "    feats_imput_max = [\"C2_Accident_gap_weeks\", \"C3_Accident_gap_weeks\", \"Accident Date_assembly_gap_days\", \"Hearing_C3 gap_months\", \"Hearing_C2 gap_months\", \"Hearing_assembly_gap_months\", \"Days to First Hearing\"]\n",
    "\n",
    "    feat_imput_min = [\"C3-C2_gap_days\"]\n",
    "    \n",
    "    for feat in X_train.columns:\n",
    "        if X_train[feat].isna().sum() > 0 or X_val[feat].isna().sum() > 0:\n",
    "            if feat in feats_imput_max:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].max())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].max())\n",
    "            elif feat in feat_imput_min:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].min())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].min())\n",
    "            else:\n",
    "                X_train[feat] = X_train[feat].fillna(X_train[feat].mean())\n",
    "                X_val[feat] = X_val[feat].fillna(X_train[feat].mean())\n",
    "    return X_train, X_val\n",
    "\n",
    "def frequency_encoding(train_df, val_df, column):\n",
    "    \"\"\"\n",
    "    Apply frequency encoding on the training set and use the same encoding to impute the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training dataset.\n",
    "    val_df (pd.DataFrame): Validation dataset.\n",
    "    column (str): Column to encode.\n",
    "    \n",
    "    Returns:\n",
    "    train_encoded (pd.DataFrame): Encoded training set.\n",
    "    val_encoded (pd.DataFrame): Encoded validation set.\n",
    "    freq_map (dict): Mapping of frequency counts for the column.\n",
    "    \"\"\"\n",
    "    # Compute frequency encoding for the training set\n",
    "    freq_map = train_df[column].value_counts(normalize=True)  # Relative frequency\n",
    "    train_df[f\"{column}_freq\"] = train_df[column].map(freq_map)\n",
    "\n",
    "    # Impute frequency encoding on the validation set using the same mapping\n",
    "    val_df[f\"{column}_freq\"] = val_df[column].map(freq_map)\n",
    "\n",
    "    # Handle unseen categories in validation by imputing 0 frequency\n",
    "    val_df[f\"{column}_freq\"] = val_df[f\"{column}_freq\"].fillna(0)\n",
    "    \n",
    "    train_df = train_df.drop(columns=[column])\n",
    "    val_df = val_df.drop(columns=[column])\n",
    "\n",
    "    # Return encoded datasets and frequency map\n",
    "    return train_df, val_df, freq_map\n",
    "\n",
    "def target_guided_ordinal_encoding(X_train, X_val, categorical_column, target_column, y_train, i):\n",
    "    # Combine X_train with y_train temporarily to calculate means\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_val_encoded = X_val.copy()\n",
    "    X_train_encoded[target_column] = y_train\n",
    "\n",
    "    means = X_train_encoded.groupby(categorical_column)[target_column].mean()\n",
    "    #print(means)\n",
    "\n",
    "    sorted_means = means.sort_values(by=target_column)\n",
    "    #print(sorted_means)\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing sorted means for {categorical_column}\")\n",
    "    #     lst_names = sorted_means.index.tolist()\n",
    "    #     lst_values = sorted_means.values.tolist()\n",
    "    #     dict_final = dict(zip(lst_names, lst_values))\n",
    "    #     print(dict_final)\n",
    "    \n",
    "    ordinal_mapping = {category: rank for rank, category in enumerate(sorted_means.index, start=1)}\n",
    "    # if i == 1:\n",
    "    #     print(f\"Showing ordinal mapping for {categorical_column}\")\n",
    "    #     print(ordinal_mapping)\n",
    "    #     print(\"--------------------------------\")\n",
    "        \n",
    "    X_train_encoded[f\"{categorical_column}_encoded\"] = X_train_encoded[categorical_column].map(ordinal_mapping)\n",
    "    X_val_encoded[f\"{categorical_column}_encoded\"] = X_val_encoded[categorical_column].map(ordinal_mapping)\n",
    "\n",
    "    #X_train_encoded = X_train_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.drop(columns=[target_column[0]])\n",
    "    #X_val_encoded = X_val_encoded.drop(columns=[categorical_column])\n",
    "    X_train_encoded = X_train_encoded.fillna(1)\n",
    "    X_val_encoded = X_val_encoded.fillna(1)\n",
    "\n",
    "    return X_train_encoded, X_val_encoded, ordinal_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_thresholds(y_true, probabilities):\n",
    "    best_thresholds = []\n",
    "    for i in range(probabilities.shape[1]):  # Loop over each class\n",
    "        precision, recall, thresholds = precision_recall_curve((y_true == i).astype(int), probabilities[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_thresholds.append(thresholds[np.argmax(f1_scores)])  # Store best threshold\n",
    "    return best_thresholds\n",
    "\n",
    "def predict_with_thresholds(probabilities, thresholds):\n",
    "    weighted_probs = probabilities / np.array(thresholds)  \n",
    "    predictions = np.argmax(weighted_probs, axis=1)  \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features=['Attorney/Representative',\n",
    " 'IME-4 Count',\n",
    " 'Accident Date_year',\n",
    " 'Accident Date_assembly_gap_days',\n",
    " 'C3-C2_gap_days',\n",
    " 'C2_missing',\n",
    " 'C3_missing',\n",
    " 'C3_Accident_gap_weeks',\n",
    " 'Hearing_C3 gap_months',\n",
    " 'Hearing_C2 gap_months',\n",
    " 'Days to Assembly',\n",
    " 'Days to First Hearing',\n",
    " 'Average Weekly Wage_log',\n",
    " 'Carrier Name_encoded',\n",
    " 'Carrier Type_encoded',\n",
    " 'Industry Code Description_encoded',\n",
    " 'WCIO Cause of Injury Description_encoded',\n",
    " 'WCIO Nature of Injury Description_encoded',\n",
    " 'WCIO Part Of Body Description_encoded',\n",
    " 'Carrier Name_freq',\n",
    " 'Carrier Type_freq',\n",
    " 'Industry Code Description_freq',\n",
    " 'WCIO Nature of Injury Description_freq',\n",
    " 'WCIO Part Of Body Description_freq']\n",
    "\n",
    "naive_features = [feat.replace(\"_encoded\", \"\") for feat in selected_features]\n",
    "naive_features = [feat.replace(f\"_freq\", \"\") for feat in naive_features]\n",
    "naive_features = set(naive_features)\n",
    "naive_features = list(naive_features)\n",
    "\n",
    "cat_feats = [feat for feat in naive_features if feat in cat_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[naive_features]\n",
    "y = df[ordinal_target]\n",
    "# # ---------------  ------------------------------------\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# X_train_encoded = X_train.copy()\n",
    "# X_val_encoded = X_val.copy()\n",
    "\n",
    "# # --------------- ------------------------------------\n",
    "X_encoded = X.copy()\n",
    "X_encoded_ = X.copy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding...\n",
      "Frequency encoding...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Ordinal encoding...\")\n",
    "X_train_encoded = X_encoded.copy()\n",
    "X_val_encoded = X_encoded_.copy()\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y, 0)\n",
    "\n",
    "print(f\"Frequency encoding...\")\n",
    "for cat in cat_feats:\n",
    "    X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "\n",
    "X_train_encoded  = X_train_encoded[selected_features]\n",
    "X_val_encoded = X_val_encoded[selected_features]\n",
    "\n",
    "X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6715820\ttotal: 467ms\tremaining: 7m 46s\n",
      "10:\tlearn: 0.5091293\ttotal: 5.38s\tremaining: 8m 4s\n",
      "20:\tlearn: 0.4094734\ttotal: 10.2s\tremaining: 7m 53s\n",
      "30:\tlearn: 0.3444584\ttotal: 15.1s\tremaining: 7m 50s\n",
      "40:\tlearn: 0.3007453\ttotal: 19.9s\tremaining: 7m 45s\n",
      "50:\tlearn: 0.2701485\ttotal: 24.7s\tremaining: 7m 39s\n",
      "60:\tlearn: 0.2484519\ttotal: 29.5s\tremaining: 7m 34s\n",
      "70:\tlearn: 0.2327412\ttotal: 34.4s\tremaining: 7m 29s\n",
      "80:\tlearn: 0.2214850\ttotal: 39.1s\tremaining: 7m 23s\n",
      "90:\tlearn: 0.2129569\ttotal: 44s\tremaining: 7m 19s\n",
      "100:\tlearn: 0.2069734\ttotal: 49s\tremaining: 7m 16s\n",
      "110:\tlearn: 0.2020307\ttotal: 54.4s\tremaining: 7m 15s\n",
      "120:\tlearn: 0.1979187\ttotal: 59.4s\tremaining: 7m 11s\n",
      "130:\tlearn: 0.1948048\ttotal: 1m 4s\tremaining: 7m 6s\n",
      "140:\tlearn: 0.1924779\ttotal: 1m 9s\tremaining: 7m 1s\n",
      "150:\tlearn: 0.1903922\ttotal: 1m 14s\tremaining: 7m 1s\n",
      "160:\tlearn: 0.1885564\ttotal: 1m 20s\tremaining: 6m 58s\n",
      "170:\tlearn: 0.1869775\ttotal: 1m 26s\tremaining: 6m 57s\n",
      "180:\tlearn: 0.1857232\ttotal: 1m 32s\tremaining: 6m 56s\n",
      "190:\tlearn: 0.1845498\ttotal: 1m 38s\tremaining: 6m 55s\n",
      "200:\tlearn: 0.1835611\ttotal: 1m 43s\tremaining: 6m 53s\n",
      "210:\tlearn: 0.1826236\ttotal: 1m 49s\tremaining: 6m 50s\n",
      "220:\tlearn: 0.1818759\ttotal: 1m 55s\tremaining: 6m 47s\n",
      "230:\tlearn: 0.1810699\ttotal: 2m\tremaining: 6m 42s\n",
      "240:\tlearn: 0.1803768\ttotal: 2m 6s\tremaining: 6m 36s\n",
      "250:\tlearn: 0.1797325\ttotal: 2m 11s\tremaining: 6m 32s\n",
      "260:\tlearn: 0.1791612\ttotal: 2m 17s\tremaining: 6m 28s\n",
      "270:\tlearn: 0.1786494\ttotal: 2m 22s\tremaining: 6m 22s\n",
      "280:\tlearn: 0.1782068\ttotal: 2m 27s\tremaining: 6m 16s\n",
      "290:\tlearn: 0.1777273\ttotal: 2m 32s\tremaining: 6m 11s\n",
      "300:\tlearn: 0.1772953\ttotal: 2m 37s\tremaining: 6m 5s\n",
      "310:\tlearn: 0.1769094\ttotal: 2m 43s\tremaining: 6m 1s\n",
      "320:\tlearn: 0.1764921\ttotal: 2m 49s\tremaining: 5m 57s\n",
      "330:\tlearn: 0.1760885\ttotal: 2m 54s\tremaining: 5m 52s\n",
      "340:\tlearn: 0.1757296\ttotal: 2m 59s\tremaining: 5m 46s\n",
      "350:\tlearn: 0.1754240\ttotal: 3m 4s\tremaining: 5m 40s\n",
      "360:\tlearn: 0.1750838\ttotal: 3m 9s\tremaining: 5m 34s\n",
      "370:\tlearn: 0.1747860\ttotal: 3m 14s\tremaining: 5m 29s\n",
      "380:\tlearn: 0.1745361\ttotal: 3m 19s\tremaining: 5m 23s\n",
      "390:\tlearn: 0.1742687\ttotal: 3m 24s\tremaining: 5m 17s\n",
      "400:\tlearn: 0.1739839\ttotal: 3m 29s\tremaining: 5m 12s\n",
      "410:\tlearn: 0.1736910\ttotal: 3m 34s\tremaining: 5m 7s\n",
      "420:\tlearn: 0.1734569\ttotal: 3m 39s\tremaining: 5m 1s\n",
      "430:\tlearn: 0.1732031\ttotal: 3m 44s\tremaining: 4m 56s\n",
      "440:\tlearn: 0.1729342\ttotal: 3m 49s\tremaining: 4m 51s\n",
      "450:\tlearn: 0.1726565\ttotal: 3m 55s\tremaining: 4m 46s\n",
      "460:\tlearn: 0.1724369\ttotal: 4m\tremaining: 4m 41s\n",
      "470:\tlearn: 0.1722237\ttotal: 4m 5s\tremaining: 4m 36s\n",
      "480:\tlearn: 0.1720037\ttotal: 4m 11s\tremaining: 4m 30s\n",
      "490:\tlearn: 0.1717906\ttotal: 4m 16s\tremaining: 4m 25s\n",
      "500:\tlearn: 0.1715771\ttotal: 4m 22s\tremaining: 4m 21s\n",
      "510:\tlearn: 0.1713751\ttotal: 4m 27s\tremaining: 4m 15s\n",
      "520:\tlearn: 0.1711984\ttotal: 4m 32s\tremaining: 4m 10s\n",
      "530:\tlearn: 0.1709975\ttotal: 4m 37s\tremaining: 4m 5s\n",
      "540:\tlearn: 0.1708297\ttotal: 4m 43s\tremaining: 4m\n",
      "550:\tlearn: 0.1706562\ttotal: 4m 48s\tremaining: 3m 55s\n",
      "560:\tlearn: 0.1704772\ttotal: 4m 53s\tremaining: 3m 49s\n",
      "570:\tlearn: 0.1702734\ttotal: 4m 59s\tremaining: 3m 44s\n",
      "580:\tlearn: 0.1701052\ttotal: 5m 4s\tremaining: 3m 39s\n",
      "590:\tlearn: 0.1699273\ttotal: 5m 10s\tremaining: 3m 34s\n",
      "600:\tlearn: 0.1697414\ttotal: 5m 15s\tremaining: 3m 29s\n",
      "610:\tlearn: 0.1695950\ttotal: 5m 21s\tremaining: 3m 24s\n",
      "620:\tlearn: 0.1694304\ttotal: 5m 25s\tremaining: 3m 18s\n",
      "630:\tlearn: 0.1693202\ttotal: 5m 30s\tremaining: 3m 13s\n",
      "640:\tlearn: 0.1691710\ttotal: 5m 36s\tremaining: 3m 8s\n",
      "650:\tlearn: 0.1690321\ttotal: 5m 42s\tremaining: 3m 3s\n",
      "660:\tlearn: 0.1689016\ttotal: 5m 49s\tremaining: 2m 59s\n",
      "670:\tlearn: 0.1687550\ttotal: 5m 57s\tremaining: 2m 55s\n",
      "680:\tlearn: 0.1686325\ttotal: 6m 2s\tremaining: 2m 49s\n",
      "690:\tlearn: 0.1685149\ttotal: 6m 7s\tremaining: 2m 44s\n",
      "700:\tlearn: 0.1683778\ttotal: 6m 12s\tremaining: 2m 38s\n",
      "710:\tlearn: 0.1682597\ttotal: 6m 17s\tremaining: 2m 33s\n",
      "720:\tlearn: 0.1681359\ttotal: 6m 22s\tremaining: 2m 28s\n",
      "730:\tlearn: 0.1680075\ttotal: 6m 27s\tremaining: 2m 22s\n",
      "740:\tlearn: 0.1678921\ttotal: 6m 32s\tremaining: 2m 17s\n",
      "750:\tlearn: 0.1677693\ttotal: 6m 37s\tremaining: 2m 11s\n",
      "760:\tlearn: 0.1676442\ttotal: 6m 42s\tremaining: 2m 6s\n",
      "770:\tlearn: 0.1675363\ttotal: 6m 47s\tremaining: 2m\n",
      "780:\tlearn: 0.1674150\ttotal: 6m 52s\tremaining: 1m 55s\n",
      "790:\tlearn: 0.1673156\ttotal: 6m 58s\tremaining: 1m 50s\n",
      "800:\tlearn: 0.1672212\ttotal: 7m 3s\tremaining: 1m 45s\n",
      "810:\tlearn: 0.1671344\ttotal: 7m 8s\tremaining: 1m 39s\n",
      "820:\tlearn: 0.1670114\ttotal: 7m 13s\tremaining: 1m 34s\n",
      "830:\tlearn: 0.1669210\ttotal: 7m 18s\tremaining: 1m 29s\n",
      "840:\tlearn: 0.1668216\ttotal: 7m 23s\tremaining: 1m 23s\n",
      "850:\tlearn: 0.1667399\ttotal: 7m 28s\tremaining: 1m 18s\n",
      "860:\tlearn: 0.1666420\ttotal: 7m 33s\tremaining: 1m 13s\n",
      "870:\tlearn: 0.1665644\ttotal: 7m 38s\tremaining: 1m 7s\n",
      "880:\tlearn: 0.1664813\ttotal: 7m 43s\tremaining: 1m 2s\n",
      "890:\tlearn: 0.1664105\ttotal: 7m 48s\tremaining: 57.3s\n",
      "900:\tlearn: 0.1663089\ttotal: 7m 53s\tremaining: 52s\n",
      "910:\tlearn: 0.1662072\ttotal: 7m 57s\tremaining: 46.7s\n",
      "920:\tlearn: 0.1661205\ttotal: 8m 2s\tremaining: 41.4s\n",
      "930:\tlearn: 0.1660279\ttotal: 8m 7s\tremaining: 36.1s\n",
      "940:\tlearn: 0.1659371\ttotal: 8m 12s\tremaining: 30.9s\n",
      "950:\tlearn: 0.1658614\ttotal: 8m 17s\tremaining: 25.6s\n",
      "960:\tlearn: 0.1657580\ttotal: 8m 23s\tremaining: 20.4s\n",
      "970:\tlearn: 0.1656802\ttotal: 8m 27s\tremaining: 15.2s\n",
      "980:\tlearn: 0.1655840\ttotal: 8m 32s\tremaining: 9.93s\n",
      "990:\tlearn: 0.1655154\ttotal: 8m 38s\tremaining: 4.71s\n",
      "999:\tlearn: 0.1654324\ttotal: 8m 43s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fb91dbb7e90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = CatBoostClassifier(random_state=42, verbose=10, iterations=1000, depth=6, boosting_type='Ordered', auto_class_weights='SqrtBalanced', loss_function=\"MultiClassOneVsAll\")\n",
    "\n",
    "clf.fit(X_train_imputed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_preprocessing(train, test, cat_feats, selected_features, y):\n",
    "    # --------------- Date times\n",
    "    for feat in test.columns:\n",
    "        if \"Date\" in feat:\n",
    "            test[feat] = pd.to_datetime(test[feat], format=\"%Y-%m-%d\")\n",
    "    date_feats = [feat for feat in test.columns if \"Date\" in feat]\n",
    "    date_feats.append(\"Birth Year\")\n",
    "    \n",
    "    \n",
    "    test[\"Birth Year\"] = np.where(test[\"Birth Year\"] == 0, test[\"Accident Date\"].dt.year - test[\"Age at Injury\"], test[\"Birth Year\"])\n",
    "    test[\"Birth Year\"] = pd.to_datetime(test[\"Birth Year\"], format=\"%Y\")\n",
    "    \n",
    "    \n",
    "    test[\"Accident Date_year\"] = test[\"Accident Date\"].dt.year\n",
    "    test[\"Accident Date_year\"] = np.where(test[\"Accident Date_year\"] < 2019, 2019, test[\"Accident Date_year\"])\n",
    "    \n",
    "    \n",
    "    test[\"Accident Date_assembly_gap_months\"] = (test[\"Assembly Date\"] - test[\"Accident Date\"]).dt.days/30\n",
    "    test[\"Accident Date\"] = np.where(test[\"Accident Date_assembly_gap_months\"] < 0, test[[\"C-2 Date\", \"C-3 Date\"]].min(axis=1), test[\"Accident Date\"])\n",
    "    test[\"Accident Date_assembly_gap_months\"] = (test[\"Assembly Date\"] - test[\"Accident Date\"]).dt.days/30\n",
    "    test[\"Assembly Date\"] = np.where(test[\"Accident Date_assembly_gap_months\"] < 0, test[\"Assembly Date\"] + pd.DateOffset(years=1), test[\"Assembly Date\"])\n",
    "    test.drop(columns=[\"Accident Date_assembly_gap_months\"], inplace=True)\n",
    "    test[\"Accident Date_assembly_gap_days\"] = (test[\"Assembly Date\"] - test[\"Accident Date\"]).dt.days\n",
    "    test[\"Accident Date_assembly_gap_days\"] = np.where(test[\"Accident Date_assembly_gap_days\"] > 30, 30, test[\"Accident Date_assembly_gap_days\"])\n",
    "    \n",
    "    \n",
    "    test[\"C3-C2_gap_days\"] = (test[\"C-3 Date\"] - test[\"C-2 Date\"]).dt.days\n",
    "    test[\"C3-C2_gap_days\"] = np.where(test[\"C3-C2_gap_days\"] < -60, -60, test[\"C3-C2_gap_days\"])\n",
    "    test[\"C3-C2_gap_days\"] = np.where(test[\"C3-C2_gap_days\"] > 60, 60, test[\"C3-C2_gap_days\"])\n",
    "    \n",
    "    \n",
    "    test[\"C3_Accident_gap_weeks\"] = ((test[\"C-3 Date\"] - test[\"Accident Date\"]).dt.days/7)\n",
    "    test[\"C3_Accident_gap_weeks\"] = np.where(test[\"C3_Accident_gap_weeks\"] < -4, -4, test[\"C3_Accident_gap_weeks\"])\n",
    "    test[\"C3_Accident_gap_weeks\"] = np.where(test[\"C3_Accident_gap_weeks\"] > 24, 24, test[\"C3_Accident_gap_weeks\"])\n",
    "    \n",
    "    \n",
    "    test[\"C3_missing\"] = np.where(test[\"C-3 Date\"].isna(), True, False)\n",
    "    \n",
    "    \n",
    "    test[\"C2_missing\"] = np.where(test[\"C-2 Date\"].isna(), True, False)\n",
    "\n",
    "\n",
    "    test[\"Hearing_C3 gap_months\"] = ((test[\"First Hearing Date\"].dt.year - test[\"C-3 Date\"].dt.year) * 12 + (test[\"First Hearing Date\"].dt.month - test[\"C-3 Date\"].dt.month))\n",
    "    test[\"Hearing_C3 gap_months\"] = np.where(test[\"Hearing_C3 gap_months\"] > 50, 50, test[\"Hearing_C3 gap_months\"])\n",
    "    test[\"Hearing_C3 gap_months\"] = np.where(test[\"Hearing_C3 gap_months\"] < -20, -20, test[\"Hearing_C3 gap_months\"])\n",
    "    \n",
    "    \n",
    "    test[\"Hearing_C2 gap_months\"] = ((test[\"First Hearing Date\"].dt.year - test[\"C-2 Date\"].dt.year) * 12 + (test[\"First Hearing Date\"].dt.month - test[\"C-2 Date\"].dt.month))\n",
    "    test[\"Hearing_C2 gap_months\"] = np.where(test[\"Hearing_C2 gap_months\"] > 50, 50, test[\"Hearing_C2 gap_months\"])\n",
    "    test[\"Hearing_C2 gap_months\"] = np.where(test[\"Hearing_C2 gap_months\"] < -20, -20, test[\"Hearing_C2 gap_months\"])\n",
    "    \n",
    "    \n",
    "    test[\"Hearing_assembly_gap_months\"] = ((test[\"First Hearing Date\"].dt.year - test[\"Assembly Date\"].dt.year) * 12 + (test[\"First Hearing Date\"].dt.month - test[\"Assembly Date\"].dt.month))\n",
    "    test[\"First Hearing Date\"] = np.where(test[\"Hearing_assembly_gap_months\"] < 0, test[\"First Hearing Date\"] + pd.DateOffset(years=1), test[\"First Hearing Date\"])\n",
    "    test[\"Hearing_assembly_gap_months\"] = ((test[\"First Hearing Date\"].dt.year - test[\"Assembly Date\"].dt.year) * 12 + (test[\"First Hearing Date\"].dt.month - test[\"Assembly Date\"].dt.month))\n",
    "    \n",
    "    \n",
    "    test[\"Days to Assembly\"] = (test[\"Assembly Date\"] - pd.to_datetime('2020-01-01 00:00:00', format=\"%Y-%m-%d %H:%M:%S\")).dt.days\n",
    "    \n",
    "        \n",
    "    test[\"Days to First Hearing\"] = (test[\"First Hearing Date\"] - pd.to_datetime('2020-01-30 00:00:00', format=\"%Y-%m-%d %H:%M:%S\")).dt.days\n",
    "    \n",
    "\n",
    "    # --------------- Numerical feats\n",
    "\n",
    "    test[\"Age at Injury\"] = np.where(test[\"Age at Injury\"] == 0, test[\"Accident Date\"].dt.year - test[\"Birth Year\"].dt.year, test[\"Age at Injury\"])\n",
    "    test[\"Birth Year\"] = np.where(test[\"Birth Year\"].isna() & ~(test[\"Accident Date\"].isna()), pd.to_datetime(test[\"Accident Date\"].dt.year - test[\"Age at Injury\"], format=\"%Y\"), test[\"Birth Year\"])\n",
    "    test[\"Age at Injury\"] = np.where(test[\"Age at Injury\"] == 0, test[\"Accident Date\"].dt.year - test[\"Birth Year\"].dt.year, test[\"Age at Injury\"])\n",
    "    test[\"Age at Injury\"] = np.where(test[\"Age at Injury\"] < 14, np.nan, test[\"Age at Injury\"])\n",
    "    test[\"Age at Injury\"] = np.where(test[\"Age at Injury\"] > 90, 90, test[\"Age at Injury\"])\n",
    "    \n",
    "    \n",
    "    test[\"IME-4 Count\"] = np.where(test[\"IME-4 Count\"].isna(), 0, test[\"IME-4 Count\"])\n",
    "    test[\"IME-4 Count\"] = np.where(test[\"IME-4 Count\"] > 12, 12, test[\"IME-4 Count\"])\n",
    "\n",
    "\n",
    "    test[\"Average Weekly Wage_log\"] = test[\"Average Weekly Wage\"].apply(lambda x: np.log1p(x))\n",
    "\n",
    "\n",
    "    # --------------- Boolean feats \n",
    "    test[\"Alternative Dispute Resolution\"] = np.where(test[\"Alternative Dispute Resolution\"] == \"U\", \"Y\", test[\"Alternative Dispute Resolution\"])\n",
    "    test[\"Alternative Dispute Resolution\"] = test[\"Alternative Dispute Resolution\"].map({\"Y\": True, \"N\": False})\n",
    "\n",
    "\n",
    "    test[\"Attorney/Representative\"] = test[\"Attorney/Representative\"].map({\"Y\": True, \"N\": False})\n",
    "\n",
    "    \n",
    "    # --------------- Categorical feats\n",
    "    train_categories = train[\"Carrier Name\"].value_counts().index\n",
    "    test[\"Carrier Name\"] = np.where(~test[\"Carrier Name\"].isin(train_categories), \"Other\", test[\"Carrier Name\"])\n",
    "\n",
    "\n",
    "    train_categories = train[\"Carrier Type\"].value_counts().index\n",
    "    test[\"Carrier Type\"] = np.where(~test[\"Carrier Type\"].isin(train_categories), \"5D. SPECIAL FUND - UNKNOWN\", test[\"Carrier Type\"])\n",
    "    \n",
    "    \n",
    "    test[\"Industry Code Description\"] = np.where(test[\"Industry Code Description\"].isna(), train[\"Industry Code Description\"].min(), test[\"Industry Code Description\"])\n",
    "    \n",
    "    \n",
    "    test[\"WCIO Cause of Injury Description\"] = np.where(test[\"WCIO Cause of Injury Description\"] == \"CRASH OF AIRPLANE\", np.nan, test[\"WCIO Cause of Injury Description\"])\n",
    "    test[\"WCIO Cause of Injury Description\"] = np.where(test[\"WCIO Cause of Injury Description\"].isna(), \"Missing\", test[\"WCIO Cause of Injury Description\"])\n",
    "    \n",
    "    \n",
    "    test[\"WCIO Nature of Injury Description\"] = np.where(test[\"WCIO Nature of Injury Description\"].isna(), \"Missing\", test[\"WCIO Nature of Injury Description\"])\n",
    "    underepresented = ['POISONING - METAL', 'SILICOSIS', 'RADIATION', 'ENUCLEATION', 'BLACK LUNG', 'VDT - RELATED DISEASES', 'HEPATITIS C', 'BYSSINOSIS']\n",
    "    test[\"WCIO Nature of Injury Description\"] = np.where(test[\"WCIO Nature of Injury Description\"].isin(underepresented), \"Other\", test[\"WCIO Nature of Injury Description\"])\n",
    "    \n",
    "    \n",
    "    test[\"WCIO Part Of Body Description\"] = np.where(test[\"WCIO Part Of Body Description\"].isna(), \"Missing\", test[\"WCIO Part Of Body Description\"])\n",
    "    \n",
    "\n",
    "    # --------------- Categorical encoding\n",
    "    print(f\"Ordinal encoding...\")\n",
    "    X_train_encoded = train.copy()\n",
    "    X_val_encoded = test.copy()\n",
    "    for cat in cat_feats:\n",
    "        X_train_encoded, X_val_encoded, ordinal_mapping = target_guided_ordinal_encoding(X_train_encoded, X_val_encoded, cat, ordinal_target, y, 0)\n",
    "        \n",
    "        \n",
    "    # --------------- Frequency encoding\n",
    "    print(f\"Frequency encoding...\")\n",
    "    for cat in cat_feats:\n",
    "        X_train_encoded, X_val_encoded, freq_map = frequency_encoding(X_train_encoded, X_val_encoded, cat)\n",
    "\n",
    "    \n",
    "    X_train_encoded  = X_train_encoded[selected_features]\n",
    "    X_val_encoded = X_val_encoded[selected_features]\n",
    "    \n",
    "    \n",
    "    # --------------- Missing values imputation\n",
    "    print(f\"Imputing missing values...\")\n",
    "    X_train_imputed, X_val_imputed = num_imputing(X_train_encoded, X_val_encoded)\n",
    "    \n",
    "    \n",
    "    return X_val_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding...\n",
      "Frequency encoding...\n",
      "Imputing missing values...\n"
     ]
    }
   ],
   "source": [
    "test_processed = test_preprocessing(df, test, cat_feats, selected_features, y)\n",
    "\n",
    "test_processed_df = pd.DataFrame(test_processed, columns=selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Attorney/Representative', 'IME-4 Count', 'Accident Date_year',\n",
       "       'Accident Date_assembly_gap_days', 'C3-C2_gap_days', 'C2_missing',\n",
       "       'C3_missing', 'C3_Accident_gap_weeks', 'Hearing_C3 gap_months',\n",
       "       'Hearing_C2 gap_months', 'Days to Assembly', 'Days to First Hearing',\n",
       "       'Average Weekly Wage_log', 'Carrier Name_encoded',\n",
       "       'Carrier Type_encoded', 'Industry Code Description_encoded',\n",
       "       'WCIO Cause of Injury Description_encoded',\n",
       "       'WCIO Nature of Injury Description_encoded',\n",
       "       'WCIO Part Of Body Description_encoded', 'Carrier Name_freq',\n",
       "       'Carrier Type_freq', 'Industry Code Description_freq',\n",
       "       'WCIO Nature of Injury Description_freq',\n",
       "       'WCIO Part Of Body Description_freq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attorney/Representative                      0\n",
       "IME-4 Count                                  0\n",
       "Accident Date_year                           0\n",
       "Accident Date_assembly_gap_days              0\n",
       "C3-C2_gap_days                               0\n",
       "C2_missing                                   0\n",
       "C3_missing                                   0\n",
       "C3_Accident_gap_weeks                        0\n",
       "Hearing_C3 gap_months                        0\n",
       "Hearing_C2 gap_months                        0\n",
       "Days to Assembly                             0\n",
       "Days to First Hearing                        0\n",
       "Average Weekly Wage_log                      0\n",
       "Carrier Name_encoded                         0\n",
       "Carrier Type_encoded                         0\n",
       "Industry Code Description_encoded            0\n",
       "WCIO Cause of Injury Description_encoded     0\n",
       "WCIO Nature of Injury Description_encoded    0\n",
       "WCIO Part Of Body Description_encoded        0\n",
       "Carrier Name_freq                            0\n",
       "Carrier Type_freq                            0\n",
       "Industry Code Description_freq               0\n",
       "WCIO Nature of Injury Description_freq       0\n",
       "WCIO Part Of Body Description_freq           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_thresholds = [0.5523639280183358,\n",
    " 0.2706995022796855,\n",
    " 0.24118589499033843,\n",
    " 0.2641656406998126,\n",
    " 0.37531474703930623,\n",
    " 0.3281035672210727,\n",
    " 0.5098517158018644,\n",
    " 0.634415672799047]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probas = clf.predict_proba(test_processed_df)\n",
    "model_probas.shape\n",
    "predictions = predict_with_thresholds(model_probas, best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OrdinalEncoder(categories=[[&#x27;1. CANCELLED&#x27;, &#x27;2. NON-COMP&#x27;, &#x27;3. MED ONLY&#x27;,\n",
       "                            &#x27;4. TEMPORARY&#x27;, &#x27;5. PPD SCH LOSS&#x27;, &#x27;6. PPD NSL&#x27;,\n",
       "                            &#x27;7. PTD&#x27;, &#x27;8. DEATH&#x27;]])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder(categories=[[&#x27;1. CANCELLED&#x27;, &#x27;2. NON-COMP&#x27;, &#x27;3. MED ONLY&#x27;,\n",
       "                            &#x27;4. TEMPORARY&#x27;, &#x27;5. PPD SCH LOSS&#x27;, &#x27;6. PPD NSL&#x27;,\n",
       "                            &#x27;7. PTD&#x27;, &#x27;8. DEATH&#x27;]])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OrdinalEncoder(categories=[['1. CANCELLED', '2. NON-COMP', '3. MED ONLY',\n",
       "                            '4. TEMPORARY', '5. PPD SCH LOSS', '6. PPD NSL',\n",
       "                            '7. PTD', '8. DEATH']])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinalencoder = OrdinalEncoder(categories=[[\"1. CANCELLED\", \"2. NON-COMP\", \"3. MED ONLY\", \"4. TEMPORARY\", \"5. PPD SCH LOSS\", \"6. PPD NSL\", \"7. PTD\", \"8. DEATH\"]])\n",
    "ordinalencoder.categories\n",
    "ordinalencoder.fit(df[[\"Claim Injury Type\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adjusted_preds = ordinalencoder.inverse_transform(predictions.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_adjuted = pd.DataFrame(model_adjusted_preds, columns=[\"Claim Injury Type\"], index=test[\"Claim Identifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_adjuted.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
